{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install -q monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'segment-anything'...\n",
      "remote: Enumerating objects: 306, done.\u001b[K\n",
      "remote: Total 306 (delta 0), reused 0 (delta 0), pack-reused 306\u001b[K\n",
      "Receiving objects: 100% (306/306), 18.31 MiB | 52.51 MiB/s, done.\n",
      "Resolving deltas: 100% (163/163), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone -b batch_size https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/av3451/my_env/NeuroSAM/segment-anything\n"
     ]
    }
   ],
   "source": [
    "# %cd segment-anything/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///scratch/av3451/my_env/NeuroSAM/segment-anything\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: segment-anything\n",
      "  Attempting uninstall: segment-anything\n",
      "    Found existing installation: segment-anything 1.0\n",
      "    Uninstalling segment-anything-1.0:\n",
      "      Successfully uninstalled segment-anything-1.0\n",
      "  Running setup.py develop for segment-anything\n",
      "Successfully installed segment-anything-1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "join = os.path.join\n",
    "from tqdm import tqdm\n",
    "from skimage import transform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from segment_anything import sam_model_registry\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# set seeds\n",
    "torch.manual_seed(2023)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/av3451/my_env\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [00:00<00:00, 128871.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def resize(path):\n",
    "  dirs = os.listdir( path )\n",
    "  for item in tqdm(dirs):\n",
    "    if os.path.isfile(path+item):\n",
    "      im = Image.open(path+item)\n",
    "      f, e = os.path.splitext(path+item)\n",
    "      imResize = im.resize((1024,1024), Image.NEAREST)\n",
    "      imResize.save(f+e, 'PNG', quality=100)\n",
    "\n",
    "label_path =  \"/scratch/av3451/my_env/NeuroSAM/new_nice/masks\"\n",
    "output_features_path = \"/scratch/av3451/my_env/NeuroSAM/new_nice/images\"\n",
    "resize(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_ids\n",
       "0   28.png\n",
       "1    7.png\n",
       "2   75.png\n",
       "3   25.png\n",
       "4   72.png"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=[]\n",
    "label_filenames = [f for f in listdir(label_path) if isfile(join(label_path, f))]\n",
    "feature_filenames = [f for f in listdir(output_features_path) if isfile(join(output_features_path, f))]\n",
    "for i in range(len(feature_filenames)):\n",
    "    # print(feature_filenames[i][1:])\n",
    "    ids.append(feature_filenames[i][1:])\n",
    "# print(ids)\n",
    "\n",
    "df = pd.DataFrame(ids ,columns=[\"file_ids\"])\n",
    "df.to_csv('full_file_ids.csv', index=False)\n",
    "\n",
    "#sanity check\n",
    "df = pd.read_csv('full_file_ids.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@16.510] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.642] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.642] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.860] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.927] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/09.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.927] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/06.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@16.971] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/03.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.020] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/00.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.075] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.247] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.251] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.491] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.544] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/08.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.549] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.559] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/05.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.565] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/02.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.565] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/09.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.565] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/06.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.565] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/03.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.565] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/00.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.881] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@17.941] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.015] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/07.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.018] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/04.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.018] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/01.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.018] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/08.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.018] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/05.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.018] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.035] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/02.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.072] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.074] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.102] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.310] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.409] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/07.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.421] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/04.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.434] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/01.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.532] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@18.536] global loadsave.cpp:248 findDecoder imread_('/scratch/av3451/my_env/NeuroSAM/new_nice/masks/0.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_ids\n",
       "0     1.png\n",
       "1     1.png\n",
       "2     1.png\n",
       "3     1.png\n",
       "4     1.png\n",
       "5     1.png\n",
       "6     1.png\n",
       "7     1.png\n",
       "8     1.png\n",
       "9    10.png\n",
       "10   10.png\n",
       "11   11.png\n",
       "12   11.png\n",
       "13   12.png\n",
       "14   12.png"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "df = pd.read_csv('/scratch/av3451/my_env/NeuroSAM/full_file_ids.csv')\n",
    "ids = df['file_ids'].tolist()\n",
    "non_empty_ids = []\n",
    "\n",
    "for file_id in ids:\n",
    "    mask_path = os.path.join(label_path, file_id)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if cv2.countNonZero(mask) > 0:\n",
    "        non_empty_ids.append(file_id)\n",
    "\n",
    "df_non_empty = pd.DataFrame(non_empty_ids, columns=[\"file_ids\"])\n",
    "df_non_empty.sort_values(by='file_ids', inplace=True)  # Sort the DataFrame by 'file_ids'\n",
    "df_non_empty.to_csv('file_ids.csv', index=False)\n",
    "\n",
    "\n",
    "dif = pd.read_csv('/scratch/av3451/my_env/NeuroSAM/file_ids.csv')\n",
    "dif.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, csv_file, bbox_shift=20):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.ids = self.df[\"file_ids\"]\n",
    "        self.img_path = \"/scratch/av3451/my_env/NeuroSAM/new_nice/images\"\n",
    "        self.mask_path = \"/scratch/av3451/my_env/NeuroSAM/new_nice/masks\"\n",
    "        self.bbox_shift = bbox_shift\n",
    "        print(f\"number of images: {len(self.ids)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image and mask using the ID from the CSV\n",
    "        img_name = f\"{self.ids[index]}\"\n",
    "        mask_name = f\"{self.ids[index]}\"\n",
    "\n",
    "        # Load and resize image to 1024x1024, then convert to RGB\n",
    "        img = Image.open(join(self.img_path, img_name)).resize((1024, 1024)).convert(\"RGB\")\n",
    "        img = np.array(img)  # Convert image to numpy array\n",
    "\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Load and resize mask to 1024x1024\n",
    "        mask = Image.open(join(self.mask_path, mask_name)).resize((1024, 1024)).convert(\"L\")\n",
    "        # mask = np.array(mask)  # Convert mask to numpy array\n",
    "\n",
    "        # Convert the shape to (3, H, W) for image and (1, H, W) for mask\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        mask = np.expand_dims(mask, axis=0)  # Add an extra dimension for the channel\n",
    "\n",
    "        label_ids = np.unique(mask)[1:]\n",
    "        # mask_binary = np.uint8(mask == random.choice(label_ids.tolist()))[1]  # only one label, (1024, 1024)\\\n",
    "        try:\n",
    "            mask_binary = np.uint8(mask == random.choice(label_ids.tolist()))[1]\n",
    "        except IndexError:\n",
    "            # print(\"***************EXCEPT****************\")\n",
    "            # Handle the case where only one label is present in the mask\n",
    "            mask_binary = np.uint8(mask == random.choice(label_ids.tolist()))\n",
    "\n",
    "\n",
    "        # print(mask_binary.shape)\n",
    "        if len(mask_binary.shape) > 3:\n",
    "            k,y_indices, x_indices,lm = np.where(mask_binary > 0)\n",
    "        else:  \n",
    "            k,y_indices, x_indices = np.where(mask_binary > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        ll,H, W = mask_binary.shape\n",
    "        x_min = max(0, x_min - random.randint(0, self.bbox_shift))\n",
    "        x_max = min(W, x_max + random.randint(0, self.bbox_shift))\n",
    "        y_min = max(0, y_min - random.randint(0, self.bbox_shift))\n",
    "        y_max = min(H, y_max + random.randint(0, self.bbox_shift))\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "        mb=mask_binary[None, :, :]\n",
    "        # print(mb.shape)\n",
    "        mb = mb.squeeze(0)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(img).float(),\n",
    "            torch.tensor(mb).long(),\n",
    "            torch.tensor(bboxes).float(),\n",
    "            img_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/scratch/av3451/my_env/NeuroSAM/checkpoint_save’: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir /scratch/av3451/my_env/NeuroSAM/checkpoint_save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0005\n",
    "batch_size = 8\n",
    "data_path = \"/scratch/av3451/my_env/NeuroSAM/new_nice/\"\n",
    "checkpoint = \"/scratch/av3451/my_env/NeuroSAM/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "work_dir = \"/scratch/av3451/my_env/NeuroSAM/checkpoint_save\"\n",
    "num_epochs = 1000\n",
    "num_workers=3\n",
    "use_wandb = 0\n",
    "use_amp = 0\n",
    "resume = \"\"\n",
    "task_name = \"NeuroSAM-ViT-1\"\n",
    "num_epochs = num_epochs\n",
    "iter_num = 0\n",
    "start_epoch = 0\n",
    "losses = []\n",
    "best_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 219\n",
      "torch.Size([8, 3, 1024, 1024]) torch.Size([8, 1, 1024, 1024]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your dataset\n",
    "tr_dataset = SegmentationDataset(csv_file='file_ids.csv',)\n",
    "tr_dataloader = DataLoader(tr_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251 / 255, 252 / 255, 30 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"blue\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "for step, (image, mask_binary, bboxes, img_name) in enumerate(tr_dataloader):\n",
    "    print(image.shape, mask_binary.shape, bboxes.shape)\n",
    "    # show the example\n",
    "    _, axs = plt.subplots(1, 2, figsize=(25, 25))\n",
    "    idx = random.randint(0, image.size(0) - 1)  # Update this line to get a valid index\n",
    "    axs[0].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    show_mask(mask_binary[idx].cpu().numpy()[0], axs[0])  # Passing the 2D mask to show_mask\n",
    "    show_box(bboxes[idx].numpy(), axs[0])\n",
    "    axs[0].axis(\"off\")\n",
    "    # set title\n",
    "    axs[0].set_title(img_name[idx])\n",
    "    idx = random.randint(0, image.size(0) - 1)  # Update this line to get a valid index\n",
    "    axs[1].imshow(image[idx].cpu().permute(1, 2, 0).numpy())\n",
    "    show_mask(mask_binary[idx].cpu().numpy()[0], axs[1])  # Passing the 2D mask to show_mask\n",
    "    show_box(bboxes[idx].numpy(), axs[1])\n",
    "    axs[1].axis(\"off\")\n",
    "    # set title\n",
    "    axs[1].set_title(img_name[idx])\n",
    "    # plt.show()\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "    plt.savefig(\"./data_sanitycheck.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuroSAM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_encoder,\n",
    "        mask_decoder,\n",
    "        prompt_encoder,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "        # freeze prompt encoder\n",
    "\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, box):\n",
    "        image_embedding = self.image_encoder(image)  # (B, 256, 64, 64)\n",
    "        # do not compute gradients for prompt encoder\n",
    "        with torch.no_grad():\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float32, device=image.device)\n",
    "            if len(box_torch.shape) == 2:\n",
    "                box_torch = box_torch[:, None, :]  # (B, 1, 4)\n",
    "\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "\n",
    "\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding,  # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        ori_res_masks = F.interpolate(\n",
    "            low_res_masks,\n",
    "            size=(image.shape[2], image.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        return ori_res_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls -a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sam_model_registry[model_type](checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuroSAM(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurosam_model = neuroSAM(\n",
    "        image_encoder=model.image_encoder,\n",
    "        mask_decoder=model.mask_decoder,\n",
    "        prompt_encoder=model.prompt_encoder,\n",
    "    ).to(device)\n",
    "\n",
    "neurosam_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameters:  93735472\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "        \"Number of total parameters: \",\n",
    "        sum(p.numel() for p in neurosam_model.parameters()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters:  4058340\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "        \"Number of trainable parameters: \",\n",
    "        sum(p.numel() for p in neurosam_model.parameters() if p.requires_grad),\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_encdec_params = neurosam_model.mask_decoder.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "        img_mask_encdec_params, lr=0.0001, weight_decay=0.01\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  219\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples: \", len(tr_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mav3451\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/av3451/my_env/wandb/run-20240430_022343-2lijx8m9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/av3451/NeuroSAM-ViT-B-1/runs/2lijx8m9' target=\"_blank\">fragrant-sponge-1</a></strong> to <a href='https://wandb.ai/av3451/NeuroSAM-ViT-B-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/av3451/NeuroSAM-ViT-B-1' target=\"_blank\">https://wandb.ai/av3451/NeuroSAM-ViT-B-1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/av3451/NeuroSAM-ViT-B-1/runs/2lijx8m9' target=\"_blank\">https://wandb.ai/av3451/NeuroSAM-ViT-B-1/runs/2lijx8m9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb:\n",
    "    import wandb\n",
    "\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project= task_name,\n",
    "        config={\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"data_path\": data_path,\n",
    "            \"model_type\": model_type,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "model_save_path = join(work_dir, task_name + \"-\" + run_id)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume is not None:\n",
    "    if os.path.isfile(resume):\n",
    "        checkpoint = torch.load(resume, map_location=device)\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "if use_amp:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO Ahead\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(\"GO Ahead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 404.9 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime! e/acc\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime! e/acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0225, Epoch: 0, Loss: 1.0375821325514052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0226, Epoch: 1, Loss: 1.0360413370309052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0226, Epoch: 2, Loss: 1.0606372820006476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0227, Epoch: 3, Loss: 1.0368817338237055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0228, Epoch: 4, Loss: 1.0366560640158478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0229, Epoch: 5, Loss: 1.0440418146274708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0230, Epoch: 6, Loss: 1.0367259537732159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0231, Epoch: 7, Loss: 1.0393619073761835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0232, Epoch: 8, Loss: 1.0812519832893654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0233, Epoch: 9, Loss: 1.0399401828094765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0234, Epoch: 10, Loss: 1.057524308010384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0234, Epoch: 11, Loss: 1.0655327174398634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:52<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20240430-0235, Epoch: 12, Loss: 1.0373675028483074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 21/28 [00:40<00:13,  1.95s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for step, (image, gt2D, boxes, _) in enumerate(tqdm(tr_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        boxes_np = boxes.detach().cpu().numpy()\n",
    "        image, gt2D = image.to(device), gt2D.to(device)\n",
    "        if use_amp:\n",
    "            ## AMP\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                neurosam_pred = neurosam_model(image, boxes_np)\n",
    "                loss = seg_loss(neurosam_pred, gt2D) + ce_loss(\n",
    "                    neurosam_pred, gt2D.float()\n",
    "                )\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            neurosam_pred = neurosam_model(image,boxes_np)\n",
    "            loss = seg_loss(neurosam_pred, gt2D) + ce_loss(neurosam_pred, gt2D.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        iter_num += 1\n",
    "\n",
    "    epoch_loss /= step\n",
    "    losses.append(epoch_loss)\n",
    "    if use_wandb:\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "    print(\n",
    "        f'Time: {datetime.now().strftime(\"%Y%m%d-%H%M\")}, Epoch: {epoch}, Loss: {epoch_loss}'\n",
    "    )\n",
    "    ## save the latest model\n",
    "    checkpoint = {\n",
    "        \"model\": neurosam_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, join(model_save_path, \"neurosam_model_latest.pth\"))\n",
    "    ## save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        checkpoint = {\n",
    "            \"model\": neurosam_model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(checkpoint, join(model_save_path, \"neurosam_model_best.pth\"))\n",
    "\n",
    "    # %% plot loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Dice + Cross Entropy Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(join(model_save_path, task_name + \"train_loss.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
